{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_set.csv')\n",
    "df_test = pd.read_csv('test_set_florida.csv')\n",
    "df = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['sentence'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 166)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 166)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(np.array(padded))\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5774, -0.4186, -0.6535,  ..., -0.5505, -0.5987, -0.3164])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states[0][:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2788, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_feat = features[:df_train.shape[0]]\n",
    "train_labels = df_train['target'].to_numpy()\n",
    "test_feat = features[df_train.shape[0]:]\n",
    "test_labels = df_test['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = trainData(torch.FloatTensor(train_feat), \n",
    "                       torch.FloatTensor(train_labels))\n",
    "## test data    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = testData(torch.FloatTensor(test_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "ANNModel(\n",
      "  (fc1): Linear(in_features=768, out_features=150, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc4): Linear(in_features=150, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        #self.relu2 = nn.ReLU()\n",
    "        \n",
    "        #self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        #self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "    \n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        \n",
    "        #out = self.fc2(out)\n",
    "        #out = self.tanh2(out)\n",
    "        \n",
    "        #out = self.fc3(out)\n",
    "        #out = self.elu3(out)\n",
    "        \n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "input_dim = 768\n",
    "hidden_dim = 150 \n",
    "output_dim = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = ANNModel(input_dim, hidden_dim, output_dim)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.02\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.27334 | Acc: 91.955\n",
      "Epoch 002: | Loss: 0.08330 | Acc: 96.727\n",
      "Epoch 003: | Loss: 0.08041 | Acc: 96.727\n",
      "Epoch 004: | Loss: 0.06620 | Acc: 96.818\n",
      "Epoch 005: | Loss: 0.06227 | Acc: 97.182\n",
      "Epoch 006: | Loss: 0.05498 | Acc: 97.409\n",
      "Epoch 007: | Loss: 0.06068 | Acc: 97.250\n",
      "Epoch 008: | Loss: 0.06689 | Acc: 97.068\n",
      "Epoch 009: | Loss: 0.04767 | Acc: 97.705\n",
      "Epoch 010: | Loss: 0.05303 | Acc: 97.455\n",
      "Epoch 011: | Loss: 0.04624 | Acc: 97.818\n",
      "Epoch 012: | Loss: 0.05424 | Acc: 97.455\n",
      "Epoch 013: | Loss: 0.04022 | Acc: 98.136\n",
      "Epoch 014: | Loss: 0.03507 | Acc: 98.182\n",
      "Epoch 015: | Loss: 0.04709 | Acc: 98.000\n",
      "Epoch 016: | Loss: 0.03766 | Acc: 98.205\n",
      "Epoch 017: | Loss: 0.03469 | Acc: 98.341\n",
      "Epoch 018: | Loss: 0.02996 | Acc: 98.795\n",
      "Epoch 019: | Loss: 0.03449 | Acc: 98.250\n",
      "Epoch 020: | Loss: 0.02702 | Acc: 98.750\n",
      "Epoch 021: | Loss: 0.02219 | Acc: 98.932\n",
      "Epoch 022: | Loss: 0.03512 | Acc: 98.591\n",
      "Epoch 023: | Loss: 0.04280 | Acc: 98.045\n",
      "Epoch 024: | Loss: 0.02553 | Acc: 98.727\n",
      "Epoch 025: | Loss: 0.02021 | Acc: 98.864\n",
      "Epoch 026: | Loss: 0.01715 | Acc: 99.159\n",
      "Epoch 027: | Loss: 0.01419 | Acc: 99.159\n",
      "Epoch 028: | Loss: 0.06291 | Acc: 97.795\n",
      "Epoch 029: | Loss: 0.04055 | Acc: 98.318\n",
      "Epoch 030: | Loss: 0.01810 | Acc: 99.364\n",
      "Epoch 031: | Loss: 0.01272 | Acc: 99.455\n",
      "Epoch 032: | Loss: 0.01991 | Acc: 98.932\n",
      "Epoch 033: | Loss: 0.01473 | Acc: 99.273\n",
      "Epoch 034: | Loss: 0.02142 | Acc: 99.045\n",
      "Epoch 035: | Loss: 0.01417 | Acc: 99.455\n",
      "Epoch 036: | Loss: 0.00951 | Acc: 99.545\n",
      "Epoch 037: | Loss: 0.00906 | Acc: 99.568\n",
      "Epoch 038: | Loss: 0.00424 | Acc: 99.909\n",
      "Epoch 039: | Loss: 0.00418 | Acc: 99.864\n",
      "Epoch 040: | Loss: 0.00684 | Acc: 99.818\n",
      "Epoch 041: | Loss: 0.00524 | Acc: 99.773\n",
      "Epoch 042: | Loss: 0.00269 | Acc: 99.909\n",
      "Epoch 043: | Loss: 0.00100 | Acc: 100.000\n",
      "Epoch 044: | Loss: 0.00058 | Acc: 100.000\n",
      "Epoch 045: | Loss: 0.00042 | Acc: 100.000\n",
      "Epoch 046: | Loss: 0.00040 | Acc: 100.000\n",
      "Epoch 047: | Loss: 0.00034 | Acc: 100.000\n",
      "Epoch 048: | Loss: 0.00033 | Acc: 100.000\n",
      "Epoch 049: | Loss: 0.00026 | Acc: 100.000\n",
      "Epoch 050: | Loss: 0.00023 | Acc: 100.000\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#len(train_labels)/(2*np.bincount(train_labels)) #class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.935 (+/- 0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pjhaveri/anaconda3/envs/ml_env/lib/python3.8/site-packages/sklearn/dummy.py:131: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  warnings.warn(\"The default value of strategy will change from \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_feat, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[565   5]\n",
      " [  5   3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_labels, y_pred_list)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.375\n",
      "Precision:  0.375\n",
      "F1-Score:  0.375\n"
     ]
    }
   ],
   "source": [
    "recall = cm[1][1]/(cm[1][1] + cm[1][0])\n",
    "precision = cm[1][1]/(cm[1][1] + cm[0][1])\n",
    "f1_score = 2/((1/recall) + (1/precision))\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"F1-Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "127\n",
      "139\n",
      "543\n",
      "552\n",
      "554\n",
      "555\n",
      "557\n",
      "Count\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "summary_indices=[]\n",
    "for idx,i in enumerate(y_pred_list):\n",
    "    if i==1:\n",
    "        count+=1\n",
    "        print(idx)\n",
    "        summary_indices.append(idx)\n",
    "print(\"Count\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "552\n",
      "553\n",
      "554\n",
      "Count\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "actual_summary_indices=[]\n",
    "for idx,i in enumerate(test_labels):\n",
    "    if i==1:\n",
    "        count+=1\n",
    "        print(idx)\n",
    "        actual_summary_indices.append(idx)\n",
    "print(\"Count\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[543, 547, 548, 549, 550, 552, 553, 554]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_summary_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 127, 139, 543, 552, 554, 555, 557]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list = [df_test['sentence'][i] for i in summary_indices]\n",
    "summary_output = ' '.join(summary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_summary_list = [df_test['sentence'][i] for i in actual_summary_indices]\n",
    "actual_summary = ' '.join(actual_summary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Florida's highway system contains 1,495\\xa0mi (2,406\\xa0km) of interstate highway, and 10,601\\xa0mi (17,061\\xa0km) of non-interstate highway, such as state highways and U.S. In 2011, there were about 9,000 retail gas stations in the state. Floridians consumed 21\\xa0million gallons of gasoline daily in 2011, ranking it third in national use behind California and Texas.Motorists have the 45th lowest rate of car insurance in the U.S. 24% are uninsured. Drivers between 15 and 19 years of age averaged 364 car crashes a year per ten thousand licensed Florida drivers in 2010. Drivers 70 and older averaged 95 per 10,000 during the same time frame. Intercity bus travel, which utilizes Florida's highway system, is provided by Greyhound, Megabus, and Amtrak Thruway Motorcoach. Before the construction of routes under the Federal Aid Highway Act of 1956, Florida began construction of a long cross-state toll road, Florida's Turnpike. The first section, from Fort Pierce south to the Golden Glades Interchange was completed in 1957.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The road crossed the St. Johns River at a narrow point called Wacca Pilatka, or the British name \"Cow Ford\", reflecting the fact that cattle were brought across the river there. In the pre-automobile era, railroads played a key role in the state\\'s development, particularly in coastal areas. In 1925, the Seaboard Air Line broke the FEC\\'s southeast Florida monopoly and extended its freight and passenger service to West Palm Beach; two years later it extended passenger service to Miami. Florida\\'s highway system contains 1,495\\xa0mi (2,406\\xa0km) of interstate highway, and 10,601\\xa0mi (17,061\\xa0km) of non-interstate highway, such as state highways and U.S. Intercity bus travel, which utilizes Florida\\'s highway system, is provided by Greyhound, Megabus, and Amtrak Thruway Motorcoach. The first section, from Fort Pierce south to the Golden Glades Interchange was completed in 1957. After a second section north through Orlando to Wildwood (near present-day The Villages), and a southward extension around Miami to Homestead, it was finished in 1974. Florida\\'s seven large hub and medium hub airports, as classified by the FAA, are the following: Florida has three NFL teams, two MLB teams, two NBA teams, two NHL teams, and two MLS teams.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeLsum'], use_stemmer=True)\n",
    "scores = scorer.score(actual_summary,summary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.45023696682464454, recall=0.536723163841808, fmeasure=0.4896907216494845),\n",
       " 'rouge2': Score(precision=0.319047619047619, recall=0.3806818181818182, fmeasure=0.3471502590673575),\n",
       " 'rougeLsum': Score(precision=0.3127962085308057, recall=0.3728813559322034, fmeasure=0.3402061855670103)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
